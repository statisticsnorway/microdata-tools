{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"microdata-tools","text":"<p>Tools for the microdata.no platform</p>"},{"location":"#installation","title":"Installation","text":"<p><code>microdata-tools</code> can be installed from PyPI using pip: <pre><code>pip install microdata-tools\n</code></pre></p>"},{"location":"#usage","title":"Usage","text":"<p>Once you have your metadata and data files ready to go, they should be named and stored like this: <pre><code>my-input-directory/\n    MY_DATASET_NAME/\n        MY_DATASET_NAME.csv\n        MY_DATASET_NAME.json\n</code></pre> The CSV file is optional in some cases.</p>"},{"location":"#package-dataset","title":"Package dataset","text":"<p>The <code>package_dataset()</code> function will encrypt and package your dataset as a tar archive. The process is as follows:</p> <ol> <li>Generate the symmetric key for a dataset.</li> <li>Encrypt the dataset data (CSV) using the symmetric key and store the encrypted file as <code>&lt;DATASET_NAME&gt;.csv.encr</code></li> <li>Encrypt the symmetric key using the asymmetric RSA public key <code>microdata_public_key.pem</code>     and store the encrypted file as <code>&lt;DATASET_NAME&gt;.symkey.encr</code></li> <li>Gather the encrypted CSV, encrypted symmetric key and metadata (JSON) file in one tar file.</li> </ol>"},{"location":"#unpackage-dataset","title":"Unpackage dataset","text":"<p>The <code>unpackage_dataset()</code> function will untar and decrypt your dataset using the <code>microdata_private_key.pem</code> RSA private key.</p> <p>The packaged file has to have the <code>&lt;DATASET_NAME&gt;.tar</code> extension. Its contents should be as follows:</p> <p><code>&lt;DATASET_NAME&gt;.json</code> : Required medata file.</p> <p><code>&lt;DATASET_NAME&gt;.csv.encr</code> : Optional encrypted dataset file.</p> <p><code>&lt;DATASET_NAME&gt;.symkey.encr</code> : Optional encrypted file containing the symmetrical key used to decrypt the dataset file. Required if the <code>.csv.encr</code> file is present.</p> <p>Decryption uses the RSA private key located at <code>RSA_KEY_DIR</code>.</p> <p>The packaged file is then stored in <code>output_dir/archive/unpackaged</code> after a successful run or <code>output_dir/archive/failed</code> after an unsuccessful run.</p>"},{"location":"#example","title":"Example","text":"<p>Python script that uses a RSA public key named <code>microdata_public_key.pem</code> and packages a dataset:</p> <pre><code>from pathlib import Path\nfrom microdata_tools import package_dataset\n\nRSA_KEYS_DIRECTORY = Path(\"tests/resources/rsa_keys\")\nDATASET_DIRECTORY = Path(\"tests/resources/input_package/DATASET_1\")\nOUTPUT_DIRECTORY = Path(\"tests/resources/output\")\n\npackage_dataset(\n   rsa_keys_dir=RSA_KEYS_DIRECTORY,\n   dataset_dir=DATASET_DIRECTORY,\n   output_dir=OUTPUT_DIRECTORY,\n)\n</code></pre>"},{"location":"#validation","title":"Validation","text":"<p>Once you have your metadata and data files ready to go, they should be named and stored like this: <pre><code>my-input-directory/\n    MY_DATASET_NAME/\n        MY_DATASET_NAME.csv\n        MY_DATASET_NAME.json\n</code></pre> Note that the filename only allows upper case letters A-Z, number 0-9 and underscores.</p> <p>Import microdata-tools in your script and validate your files: <pre><code>from microdata_tools import validate_dataset\n\nvalidation_errors = validate_dataset(\n    \"MY_DATASET_NAME\",\n    input_directory=\"path/to/my-input-directory\"\n)\n\nif not validation_errors:\n    print(\"My dataset is valid\")\nelse:\n    print(\"Dataset is invalid :(\")\n    # You can print your errors like this:\n    for error in validation_errors:\n        print(error)\n</code></pre></p> <p>For a more in-depth explanation of usage visit the usage documentation.</p>"},{"location":"#data-format-description","title":"Data format description","text":"<p>A dataset as defined in microdata consists of one data file, and one metadata file.</p> <p>The data file is a csv file seperated by semicolons. A valid example would be: <pre><code>000000000000001;123;2020-01-01;2020-12-31;\n000000000000002;123;2020-01-01;2020-12-31;\n000000000000003;123;2020-01-01;2020-12-31;\n000000000000004;123;2020-01-01;2020-12-31;\n</code></pre></p> <p>The metadata files should be in json format. The requirements for the metadata is best described through the Pydantic model and  the examples</p>"},{"location":"USAGE/","title":"USAGE","text":""},{"location":"USAGE/#get-started","title":"Get started","text":"<p>Install microdata-tools through pip: <pre><code>pip install microdata-tools\n</code></pre></p> <p>Upgrade to a newer version <pre><code>pip install microdata-tools --upgrade\n</code></pre></p> <p>Import into your python script like so: <pre><code>import microdata_tools\n</code></pre></p>"},{"location":"USAGE/#validate-dataset","title":"Validate dataset","text":"<p>Once you have your metadata and data files ready to go, they should be named and stored like this: <pre><code>my-input-directory/\n    MY_DATASET_NAME/\n        MY_DATASET_NAME.csv\n        MY_DATASET_NAME.json\n    MY_OTHER_DATASET/\n        MY_OTHER_DATASET.json\n</code></pre></p> <p>Import microdata-tools in your script and validate your files:</p> <pre><code>from microdata_tools import validate_dataset\n\nvalidation_errors = validate_dataset(\"MY_DATASET_NAME\")\n\nif not validation_errors:\n    print(\"My dataset is valid\")\nelse:\n    print(\"Dataset is invalid :(\")\n    # You can print your errors like this:\n    for error in validation_errors:\n        print(error)\n</code></pre> <p>The input directory is set to the directory of the script by default. If you wish to use a different directory, you can use the <code>input_directory</code>-parameter:</p> <pre><code>from microdata_tools import validate_dataset\n\nvalidation_errors = validate_dataset(\n    \"MY_DATASET_NAME\",\n    input_directory=\"/my/input/directory\",\n)\n\nif not validation_errors:\n    print(\"My dataset is valid\")\nelse:\n    print(\"Dataset is invalid :(\")\n</code></pre> <p>The validate function will temporarily generate some files in order to validate your dataset. To do this, it will create a working directory in the same location as your script, and delete it once it is done. Therefore, it is important that you have writing permissions in your directory. You can also choose to define the location of this directory yourself using the <code>working_directory</code>-parameter. If you choose to do this, the validate function will only delete the files it generates.</p> <pre><code>from microdata_tools import validate_dataset\n\nvalidation_errors = validate_dataset(\n    \"my-dataset-name\",\n    input_directory=\"/my/input/directory\",\n    working_directory=\"/my/working/directory\"\n)\n\nif not validation_errors:\n    print(\"My dataset is valid\")\nelse:\n    print(\"Dataset is invalid :(\")\n</code></pre> <p>If you wish to keep the temporary files after the validation has run, you can do this with the <code>keep_temporary_files</code>-parameter:</p> <pre><code>from microdata_tools import validate_dataset\n\nvalidation_errors = validate_dataset(\n    \"MY_DATASET_NAME\",\n    input_directory=\"/my/input/directory\",\n    working_directory=\"/my/working/directory\",\n    keep_temporary_files=True\n)\n\nif not validation_errors:\n    print(\"My dataset is valid\")\nelse:\n    print(\"Dataset is invalid :(\")\n</code></pre>"},{"location":"USAGE/#validate-metadata","title":"Validate metadata","text":"<p>What if your data is not yet done, but you want to start generating and validating your metadata? Keep your files in the same directory structure as described above, minus the csv file. You can validate the metadata by itself with the validate_metadata function: <pre><code>from microdata_tools import validate_metadata\n\nvalidation_errors = validate_metadata(\n    \"MY_DATASET_NAME\",\n    input_directory=\"my/input/directory\"\n)\n\nif not validation_errors:\n    print(\"Metadata looks good\")\nelse:\n    print(\"Invalid metadata :(\")\n</code></pre> This will only check if all required fields are present, and that the metadata follows the correct structure. Since it does not have the data file it can not do the more complex validations. It may still be a helpful way to discover errors early.</p>"},{"location":"metadata-model/","title":"THE METADATA MODEL","text":"<p>In addition to the examples of metadata json files present in this repository, this document briefly describes the fields in the metadata model.</p>"},{"location":"metadata-model/#root-level-fields","title":"Root level fields","text":"<p>These fields describe the dataset as a whole.</p> <p>temporalityType (required): The temporality type of the dataset. Must be one of:  </p> <p><pre><code>\"temporalityType\": \"FIXED\" | \"ACCUMULATED\" | \"STATUS\" | \"EVENT\",\n</code></pre> sensitivityLevel (required): The sensitivity of the data in the dataset. Must be one of:</p> <pre><code>\"sensitivityLevel\": \"PERSON_GENERAL\" | \"PERSON_SPECIAL\" | \"PUBLIC\" | \"NONPUBLIC\",\n</code></pre> <ul> <li>PERSON_GENERAL: General personal data, this category applies to information that is generally handled without further notification and is not especially sensitive. Email address is an example.</li> <li>PERSON_SPECIAL: Special category of personal data, this is a category of data that is more sensitive. Health information is an example.</li> <li>PUBLIC: Data that is publicly available</li> <li>NONPUBLIC: Data that is not publicly available</li> </ul> <p>populationDescription (required): Description of the dataset's population. <pre><code>\"populationDescription\": [\n    {\n        \"languageCode\": \"no\",\n        \"value\": \"Alle personer registrert bosatt i Norge\"\n    }\n],\n</code></pre></p> <p>spatialCoverageDescription (optional): The geographic area relevant to the data. <pre><code>\"spatialCoverageDescription\": [{\"languageCode\": \"no\", \"value\": \"Norge\"}],\n</code></pre></p> <p>subjectFields (required): Tag(s). <pre><code>\"subjectFields\": [\n    [{\"languageCode\": \"no\", \"value\": \"BEFOLKNING\"}],\n    [{\"languageCode\": \"no\", \"value\": \"SAMFUNN\"}]\n],\n</code></pre></p>"},{"location":"metadata-model/#datarevision","title":"Datarevision","text":"<p>These fields describe the current version of the dataset.</p> <ul> <li>description (required): Description of this version of the dataset.  </li> <li>temporalEnd (optional): Description of why this dataset will not be updated anymore. Successor datasets can be optionally specified. </li> </ul> <pre><code>\"dataRevision\": {\n    \"description\": [{\"languageCode\": \"no\", \"value\": \"Nye \u00e5rganger.\"}],\n    \"temporalEnd\": {\n        \"description\": [\n            {\n                \"languageCode\": \"no\",\n                \"value\": \"Videre oppdateringer utg\u00e5r pga...\"\n            }\n        ],\n        \"successors\": \"Navn p\u00e5 erstatter\",\n    }\n},\n</code></pre>"},{"location":"metadata-model/#identifier-variables","title":"Identifier variables","text":"<p>Description of the indentifier column of the dataset. It is represented as a list in the metadata model, but currently only one identifier is allowed per dataset. The identifiers are always based on a unit. A unit is centrally defined to make joining datasets across datastores easy.</p> <ul> <li>unitType (required): The unitType for this dataset identifier column. Must be one of: </li> </ul> <pre><code>\"identifierVariables\": [\n    {\n        \"unitType\": \"FAMILIE\" | \"FORETAK\" | \"HUSHOLDNING\" | \"JOBB\" |\n                    \"KJORETOY\" | \"KOMMUNE\" | \"KURS\" | \"PERSON\" | \"BEDRIFT\"\n    }\n],\n</code></pre>"},{"location":"metadata-model/#measure-variables","title":"Measure variables","text":"<p>Description of the measure column of the dataset. It is represented as a list in the metadata model, but currently only one measure is allowed per dataset.</p> <p>If the measure column in your dataset is in fact a unit type (PERSON (FNR), BEDRIFT (ORGNR) etc.), the metadatamodel for the measure variable is different than described below. You should skip to the section Measure variables (with unitType). </p> <ul> <li>name (required): Human readable name(Label) of the measure column. This should be similar to your dataset name. Example for PERSON_INNTEKT.json: \"Person inntekt\".</li> <li>description (required): Description of the column contents. Example: \"Skattepliktig og skattefritt utbytte i... \"</li> <li>dataType  (required): DataType for the values in the column. One of: [\"STRING\", \"LONG\", \"DOUBLE\", \"DATE\"]</li> <li>format  (optional): More detailed description of the values. For example if dataType for the measure is DATE, you can specify YYYYMM, YYYYMMDD etc.</li> <li>uriDefinition  (optional): Link to external resource describing the measure.</li> <li>valueDomain (required): See definition below.</li> </ul> <pre><code>\"measureVariables\": [\n    {\n        \"name\": [{\"languageCode\": \"no\", \"value\": \"Person inntekt\"}],\n        \"description\": [\n            {\n                \"languageCode\": \"no\",\n                \"value\": \"Personens rapporterte inntekt\"\n            }\n        ],\n        \"dataType\": \"DOUBLE\",\n        \"valueDomain\": {...}\n    }\n]\n</code></pre>"},{"location":"metadata-model/#value-domain","title":"Value domain","text":"<p>Describes the Value domain for the relevant variable. Either by codeList (enumerated value domain), or a description of expected values (described value domain).</p> <ul> <li>description (required in described value domain): A description of the domain. Example for the variable \"BRUTTO_INNTEKT\": \"Alle positive tall\".</li> <li>measurementUnitDescription (required in described value domain): A description of the unit measured. Example: \"Norske Kroner\"</li> <li>measurementType (required in described value domain): A machine readable definisjon of the unit measured. One of: [CURRENCY, WEIGHT, LENGTH, HEIGHT, GEOGRAPHICAL]</li> <li>uriDefinition  (optional): Link to external resource describing the domain.</li> <li>codeList (required in enumerated value domain): A code list of valid codes for the domain, description, and their validity period. The metadata fields for each item in the codelist are: <ul> <li>code  (required): The code itself. Example: \"0301\"</li> <li>categoryTitle  (required): The category name of the code. Example: \"Oslo\"</li> <li>validFrom  (required): The code is valid from date YYYY-MM-DD  </li> <li>validUntil  (optional): The code is valid until date YYYY-MM-DD</li> </ul> </li> <li>sentinelAndMissingValues (optional in enumerted value domain): A code list where the codes represent missing or sentinel values that, while not entirely valid, are still expected to appear in the dataset.<ul> <li>code  (required): The code itself. Example: 0</li> <li>categoryTitle  (required): The category name of the code. Example: \"Unknown value\"</li> </ul> </li> </ul> <p>Here is an example of two different value domains. The first value domain belongs to a measure for dataset where the measure is a persons accumulated gross income: <pre><code>\"valueDomain\": {\n    \"uriDefinition\": [],\n    \"description\": [{\"languageCode\": \"no\", \"value\": \"Norske Kroner\"}],\n    \"measurementType\": \"CURRENCY\",\n    \"measurementUnitDescription\": [{\"languageCode\": \"no\", \"value\": \"Norske Kroner\"}],\n}\n</code></pre> This example is what we would call a described value domain.</p> <p>The second example belongs to the measure variable of a dataset where the measure describes the sex of a population: <pre><code>\"valueDomain\": {\n    \"uriDefinition\": [],\n    \"codeList\": [\n        {\n            \"code\": \"1\",\n            \"categoryTitle\": [{\"languageCode\": \"no\", \"value\": \"Mann\"}],\n            \"validFrom\": \"1900-01-01\"\n        },\n        {\n            \"code\": \"2\",\n            \"categoryTitle\": [{\"languageCode\": \"no\", \"value\": \"Kvinne\"}],\n            \"validFrom\": \"1900-01-01\"\n        }\n    ],\n    \"sentinelAndMissingValues\": [\n        {\n            \"code\": \"0\",\n            \"categoryTitle\": [{\"languageCode\": \"no\", \"value\": \"Ukjent\"}]\n        }\n    ]\n}\n</code></pre> We expect all values in this dataset to be either \"1\" or \"2\", as this dataset only considers \"Male\" or \"Female\". But we also expect a code \"0\" to be present in the dataset, where it represents \"Unknown\". A row with \"0\" as measure is therefore not considered invalid. A value domain with a code list like this is what we would call an enumerated value domain.</p>"},{"location":"metadata-model/#measure-variables-with-unittype","title":"Measure variables (with unitType)","text":"<p>You might find that some of your datasets contain a unitType in the measure column as well. Let's say you have a dataset PERSON_MOR where the identifier column is a population of unitType \"PERSON\", and the measure column is a population of unitType \"PERSON\". The measure here is representing the populations mothers. Then you may define it as such:</p> <ul> <li>unitType  (required): The unitType for this dataset measure column. Must be one of the predefined unit types.</li> <li>name (required): Human readable name(Label) of the measure column. This should be similar to your dataset name. Example for PERSON_MOR.json: \"Person mor\".</li> <li>description (required): Description of the column contents. Example: \"Personens registrerte biologiske mor...\"</li> </ul> <pre><code>\"measureVariables\": [\n    {\n        \"unitType\": \"PERSON\",\n        \"name\": [{\"languageCode\": \"no\", \"value\": \"Person mor\"}],\n        \"description\": [\n            {\"languageCode\": \"no\", \"value\": \"Personens registrerte biologiske mor\"}\n        ]\n    }\n]\n</code></pre>"},{"location":"metadata-model/#unit-types","title":"Unit types","text":"<ul> <li>PERSON: Representation of a person in the microdata.no platform. Columns with this unit type should contain FNR.</li> <li>FAMILIE: Representation of a family in the microdata.no platform. Columns with this unit type should contain FNR.</li> <li>FORETAK: Representation of a foretak in the microdata.no platform. Columns with this unit type should contain ORGNR.</li> <li>BEDRIFT: Representation of a bedrift in the microdata.no platform. Columns with this unit type should contain ORGNR.</li> <li>HUSHOLDNING: Representation of a husholdning in the microdata.no platform. Columns with this unit type should contain FNR.</li> <li>JOBB: Representation of a job in the microdata.no platform. Columns with this unit type should contain FNR_ORGNR. FNR belongs to the employee and ORGNR belongs to the employer.</li> <li>KOMMUNE: Representation of a kommune in the microdata.no platform. Columns with this unit type should contain a valid kommune number.</li> <li>KURS: Representation of a course in the microdata.no platform. Columns with this unit type should contain FNR_KURSID. Where FNR belongs to the participant and KURSID is the NUDB course id.</li> <li>KJORETOY: Representation of a vehicle in the microdata.no platform. Columns with this unit type should contain FNR_REGNR. Where FNR is the owner of the vehicle, and REGNR is the registration number for the vehicle.</li> </ul>"},{"location":"metadata-model/#validation","title":"Validation","text":""},{"location":"metadata-model/#creating-a-datafile","title":"Creating a datafile","text":"<p>A data file must be supplied as a csv file with semicolon as the column seperator. There must always be 5 columns present in this order: 1. identifier 2. measure 3. start 4. stop 5. empty column (This column is reserved for an extra attribute variable if that is considered necessary. Example: Datasource)</p> <p>Example: <pre><code>12345678910;100000;2020-01-01;2020-12-31;\n12345678910;200000;2021-01-01;2021-12-31;\n12345678911;100000;2018-01-01;2018-12-31;\n12345678911;150000;2020-01-01;2020-12-31;\n</code></pre></p> <p>This dataset describes a group of persons gross income accumulated yearly. The columns can be described like this: * Identifier: FNR * Measure: Accumulated gross income for the time period * Start: start of time period * Stop: end of time period * Empty column (This column is reserved for an extra attribute variable if that is considered necessary. As there is no need here, it remains empty.)</p>"},{"location":"metadata-model/#general-validation-rules-for-data","title":"General validation rules for data","text":"<ul> <li>There can be no empty rows in the dataset</li> <li>There can be no more than 5 elements in a row</li> <li>Every row must have a non-empty identifier</li> <li>Every row must have a non-empty measure</li> <li>Values in the stop- and start-columns must be formatted correctly: \"YYYY-MM-DD\". Example \"2020-12-31\".</li> <li>The data file must be utf-8 encoded</li> </ul>"},{"location":"metadata-model/#validation-rules-by-temporality-type","title":"Validation rules by temporality type","text":"<ul> <li>FIXED (Constant value, ex.: place of birth)<ul> <li>All rows must have an unique identifier. (No repeating identifiers within a dataset)</li> <li>All rows must have a stop date</li> </ul> </li> <li>STATUS (measurement taken at a certain point in time. (cross section))<ul> <li>All rows must have a start date</li> <li>All rows must have a stop date</li> <li>Start and stop date must be equal for any given row</li> </ul> </li> <li>ACCUMULATED (Accumulated over a period. Ex.: yearly income)<ul> <li>All rows must have a start date</li> <li>All rows must have a stop date</li> <li>Start can not be later than stop</li> <li>Time periods for the same identifiers must not intersect</li> </ul> </li> <li>EVENT (data state for validity period)<ul> <li>All rows must have a start date</li> <li>If there is a non-empty value in the stop column for a given row; start can not be later than stop</li> <li>Time periods for the same identifiers must not intersect (A row without a stop date is considered an ongoing event, and will intersect with all timespans after its start date)</li> </ul> </li> </ul>"},{"location":"issue_templates/issue_template_en/","title":"Issue template EN","text":""},{"location":"issue_templates/issue_template_en/#template-for-reporting-an-issue-with-microdata-tools","title":"Template for Reporting an Issue with microdata-tools","text":"<p>Before you report an issue:</p> <ol> <li>Check which version of microdata-tools you are using:</li> <li>On Mac:      <pre><code>pip show microdata-tools | grep Version\n</code></pre></li> <li> <p>On Windows:      <pre><code>pip show microdata-tools | findstr Version\n</code></pre></p> </li> <li> <p>Ensure you are running the latest version of <code>microdata-tools</code>. You can check the latest available version on the PyPI microdata-tools page.</p> </li> <li> <p>To upgrade to the latest version, use the following command:      <pre><code>pip install --upgrade microdata-tools\n</code></pre></p> </li> </ol> <p>If you have followed the steps above and still experience issues, please fill out the following information form and submit it as part of your problem report:</p>"},{"location":"issue_templates/issue_template_en/#information-form","title":"Information Form","text":"<ul> <li> <p>Version of <code>microdata-tools</code> being used:   (Please provide the version you have installed)</p> </li> <li> <p>Description of the problem:   (Provide a clear and detailed description of the problem. Include any error messages, warnings, or unusual behaviors that you observe.   But be careful not to include any sensitive data.)</p> </li> </ul> <p>Send us the form over email, or in any other channel where you communicate with the team.</p>"},{"location":"issue_templates/issue_template_no/","title":"Issue template NO","text":""},{"location":"issue_templates/issue_template_no/#mal-for-a-rapportere-et-problem-med-microdata-tools","title":"Mal for \u00e5 rapportere et problem med microdata-tools","text":"<p>F\u00f8r du rapporterer et problem:</p> <ol> <li>Sjekk hvilken versjon av microdata-tools du bruker:</li> <li>P\u00e5 Mac:      <pre><code>pip show microdata-tools | grep Version\n</code></pre></li> <li> <p>P\u00e5 Windows:      <pre><code>pip show microdata-tools | findstr Version\n</code></pre></p> </li> <li> <p>S\u00f8rg for at du kj\u00f8rer den nyeste versjonen av <code>microdata-tools</code>. Du kan sjekke den siste tilgjengelige versjonen p\u00e5 PyPI microdata-tools siden.</p> </li> <li> <p>For \u00e5 oppgradere til den nyeste versjonen, bruk f\u00f8lgende kommando:      <pre><code>pip install --upgrade microdata-tools\n</code></pre></p> </li> </ol> <p>Hvis du har fulgt trinnene ovenfor og fortsatt opplever problemer, vennligst fyll ut f\u00f8lgende informasjonsskjema og send det som en del av din problemrapport:</p>"},{"location":"issue_templates/issue_template_no/#informasjonsskjema","title":"Informasjonsskjema","text":"<ul> <li> <p>Versjon av <code>microdata-tools</code> som brukes:   (Oppgi versjonen du har installert)</p> </li> <li> <p>Beskrivelse av problemet:   (Gi en klar og detaljert beskrivelse av problemet. Inkluder eventuelle feilmeldinger, advarsler eller unormale oppf\u00f8rsler som du observerer.   Men pass p\u00e5 \u00e5 ikke inkludere noe sensitiv data)</p> </li> </ul> <p>Send oss skjemaet p\u00e5 epost, eller i en annen kanal du har kontakt med microdata teamet.</p>"}]}